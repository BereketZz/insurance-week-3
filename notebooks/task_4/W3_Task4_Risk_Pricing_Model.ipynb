{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eCZfXmjAsLyK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import shap\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "QA0RjyIitXOv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "file_path = \"MachineLearningRating_v3.txt\"\n",
        "df = pd.read_csv(file_path, sep=\"|\")"
      ],
      "metadata": {
        "id": "jDOzV0lRta7i"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ DATA PREPARATION ------------------\n",
        "\n",
        "# Drop rows with missing target variables\n",
        "df = df.dropna(subset=['TotalClaims', 'CalculatedPremiumPerTerm'])\n",
        "\n",
        "# Handle missing values for predictors\n",
        "df.fillna(df.median(numeric_only=True), inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "df = pd.get_dummies(df, drop_first=True)"
      ],
      "metadata": {
        "id": "SW8TOkcRwA0h"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature engineering\n",
        "df['margin'] = df['TotalPremium'] - df['TotalClaims']\n",
        "df['claim_flag'] = df['TotalClaims'].apply(lambda x: 1 if x > 0 else 0)"
      ],
      "metadata": {
        "id": "s17vHst3wFck"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset for severity prediction (only rows where claims > 0)\n",
        "df_severity = df[df['TotalClaims'] > 0].copy()"
      ],
      "metadata": {
        "id": "7uMh9K4SwHo4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# ------------------ SEVERITY MODELING ------------------\n",
        "\n",
        "# Features and Target\n",
        "X_sev = df_severity.drop(['TotalClaims', 'CalculatedPremiumPerTerm', 'claim_flag'], axis=1)\n",
        "y_sev = df_severity['TotalClaims']\n",
        "\n",
        "# Train-test split\n",
        "X_train_sev, X_test_sev, y_train_sev, y_test_sev = train_test_split(X_sev, y_sev, test_size=0.2, random_state=42)\n",
        "\n",
        "# Imputation strategy (you can use 'mean', 'median', or 'most_frequent')\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "# Models wrapped in a pipeline with imputer\n",
        "models_sev = {\n",
        "    'LinearRegression': make_pipeline(imputer, LinearRegression()),\n",
        "    'RandomForest': make_pipeline(imputer, RandomForestRegressor(random_state=42)),\n",
        "    'XGBoost': make_pipeline(imputer, XGBRegressor(random_state=42))\n",
        "}\n",
        "\n",
        "print(\"\\n--- Claim Severity Prediction Results ---\")\n",
        "for name, model in models_sev.items():\n",
        "    model.fit(X_train_sev, y_train_sev)\n",
        "    preds = model.predict(X_test_sev)\n",
        "    mse = mean_squared_error(y_test_sev, preds)\n",
        "    rmse = np.sqrt(mse)  # Manually compute RMSE\n",
        "    r2 = r2_score(y_test_sev, preds)\n",
        "    print(f\"{name} -> RMSE: {rmse:.2f}, R^2: {r2:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRr3RncUwL9n",
        "outputId": "925111a6-fc40-4ee9-ead0-9a5c25e75db7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Claim Severity Prediction Results ---\n",
            "LinearRegression -> RMSE: 0.00, R^2: 1.00\n",
            "RandomForest -> RMSE: 4033.20, R^2: 0.99\n",
            "XGBoost -> RMSE: 5184.11, R^2: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_encoded = pd.get_dummies(df, drop_first=True)\n",
        "X_clf = df_encoded.drop(['TotalClaims', 'CalculatedPremiumPerTerm'], axis=1, errors='ignore')\n",
        "y_clf = df_encoded['claim_flag']\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NaRwzZSit2-K"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ CLAIM PROBABILITY (CLASSIFICATION) ------------------\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# --------- STEP 1: Prepare target and features ---------\n",
        "# Assuming df is already loaded and has 'claim_flag' and 'TotalClaims'\n",
        "\n",
        "# Drop unwanted target-related columns\n",
        "X_clf = df.drop(['TotalClaims', 'CalculatedPremiumPerTerm'], axis=1, errors='ignore')\n",
        "y_clf = df['claim_flag']\n",
        "\n",
        "# Encode categorical columns\n",
        "for col in X_clf.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    X_clf[col] = le.fit_transform(X_clf[col].astype(str))\n",
        "\n",
        "# --------- STEP 2: Clean data ---------\n",
        "# Convert nested or invalid objects to numeric (coerce errors into NaN)\n",
        "X_clf = X_clf.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Drop rows where y_clf is NaN (just in case)\n",
        "X_clf = X_clf[y_clf.notna()]\n",
        "y_clf = y_clf[y_clf.notna()]\n",
        "\n",
        "# Impute missing values with column means\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_clf = pd.DataFrame(imputer.fit_transform(X_clf), columns=X_clf.columns)\n",
        "\n",
        "# --------- STEP 3: Sample (optional: reduce data size to avoid crashes) ---------\n",
        "X_clf_sampled = X_clf.sample(n=2000, random_state=42) if len(X_clf) > 2000 else X_clf\n",
        "y_clf_sampled = y_clf.loc[X_clf_sampled.index]\n",
        "\n",
        "# --------- STEP 4: Train-test split ---------\n",
        "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
        "    X_clf_sampled, y_clf_sampled, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# --------- STEP 5: Model definitions ---------\n",
        "models_clf = {\n",
        "    'LogisticRegression': LogisticRegression(max_iter=500, solver='liblinear'),\n",
        "    'RandomForestClassifier': RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42),\n",
        "    'XGBoostClassifier': XGBClassifier(n_estimators=50, max_depth=6, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "}\n",
        "\n",
        "# --------- STEP 6: Train and Evaluate ---------\n",
        "print(\"\\n--- Claim Probability Prediction Results ---\")\n",
        "for name, model in models_clf.items():\n",
        "    model.fit(X_train_clf, y_train_clf)\n",
        "    preds = model.predict(X_test_clf)\n",
        "    acc = accuracy_score(y_test_clf, preds)\n",
        "    prec = precision_score(y_test_clf, preds)\n",
        "    rec = recall_score(y_test_clf, preds)\n",
        "    f1 = f1_score(y_test_clf, preds)\n",
        "    print(f\"{name} -> Acc: {acc:.2f}, Prec: {prec:.2f}, Recall: {rec:.2f}, F1: {f1:.2f}\")\n"
      ],
      "metadata": {
        "id": "jydTLbflwRyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ SHAP INTERPRETABILITY ------------------\n",
        "\n",
        "# Fit best severity model (e.g., XGBoost)\n",
        "best_model = XGBRegressor(random_state=42)\n",
        "best_model.fit(X_train_sev, y_train_sev)\n",
        "explainer = shap.Explainer(best_model)\n",
        "shap_values = explainer(X_test_sev)"
      ],
      "metadata": {
        "id": "MXbdzHhrwh9K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "b328508d-522b-4ff7-8476-06ff81695702"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'XGBRegressor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3249365885>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Fit best severity model (e.g., XGBoost)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_sev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_sev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'XGBRegressor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot SHAP summary\n",
        "shap.summary_plot(shap_values, X_test_sev, max_display=10)"
      ],
      "metadata": {
        "id": "9DSDrIW2wljQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ BUSINESS INSIGHT (Example) ------------------\n",
        "print(\"\\nExample SHAP Insight:\")\n",
        "print(\"The SHAP analysis shows that VehicleAge, VehicleType_SUV, and ClientAge are among the top features influencing TotalClaims. This suggests our pricing model should place higher risk loading on older vehicles and SUV-type vehicles.\")"
      ],
      "metadata": {
        "id": "nBU6RFo5wpQ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}